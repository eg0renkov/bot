"""
–ú–æ–¥—É–ª—å –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ SERP API
"""
import asyncio
import aiohttp
import logging
from typing import List, Dict, Any, Optional
from config.settings import settings

logger = logging.getLogger(__name__)

class WebSearcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –≤–µ–±-–ø–æ–∏—Å–∫–∞ —á–µ—Ä–µ–∑ SERP API"""
    
    def __init__(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–æ–≤–∏–∫–∞"""
        self.api_key = settings.SERP_API_KEY
        self.base_url = "https://serpapi.com/search.json"
        self.session = None
    
    async def __aenter__(self):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—Ö–æ–¥"""
        self.session = aiohttp.ClientSession()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –º–µ–Ω–µ–¥–∂–µ—Ä - –≤—ã—Ö–æ–¥"""
        if self.session:
            await self.session.close()
    
    async def search(
        self,
        query: str,
        num_results: int = 10,
        location: str = "Russia",
        language: str = "ru"
    ) -> List[Dict[str, Any]]:
        """
        –í—ã–ø–æ–ª–Ω–∏—Ç—å –≤–µ–±-–ø–æ–∏—Å–∫
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            num_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            location: –õ–æ–∫–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞
            language: –Ø–∑—ã–∫ –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞
        """
        if not self.api_key:
            logger.error("SERP API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return []
        
        try:
            params = {
                'q': query,
                'api_key': self.api_key,
                'engine': 'google',
                'num': num_results,
                'location': location,
                'hl': language,
                'gl': 'ru'
            }
            
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            async with self.session.get(self.base_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_results(data)
                else:
                    logger.error(f"–û—à–∏–±–∫–∞ SERP API: {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤–µ–±-–ø–æ–∏—Å–∫–∞: {e}")
            return []
    
    def _parse_results(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã SERP API
        
        Args:
            data: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç API
            
        Returns:
            –°–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        """
        results = []
        
        # –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
        organic_results = data.get('organic_results', [])
        for result in organic_results:
            parsed_result = {
                'title': result.get('title', ''),
                'link': result.get('link', ''),
                'snippet': result.get('snippet', ''),
                'source': 'organic',
                'position': result.get('position', 0)
            }
            results.append(parsed_result)
        
        # –ù–æ–≤–æ—Å—Ç–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å)
        news_results = data.get('news_results', [])
        for result in news_results:
            parsed_result = {
                'title': result.get('title', ''),
                'link': result.get('link', ''),
                'snippet': result.get('snippet', ''),
                'source': 'news',
                'date': result.get('date', ''),
                'thumbnail': result.get('thumbnail', '')
            }
            results.append(parsed_result)
        
        # –û—Ç–≤–µ—Ç—ã (featured snippets)
        answer_box = data.get('answer_box')
        if answer_box:
            parsed_result = {
                'title': answer_box.get('title', ''),
                'link': answer_box.get('link', ''),
                'snippet': answer_box.get('snippet', ''),
                'source': 'answer_box',
                'type': answer_box.get('type', '')
            }
            results.insert(0, parsed_result)  # –°—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
        
        # –ë–ª–æ–∫ –∑–Ω–∞–Ω–∏–π
        knowledge_graph = data.get('knowledge_graph')
        if knowledge_graph:
            parsed_result = {
                'title': knowledge_graph.get('title', ''),
                'description': knowledge_graph.get('description', ''),
                'source': 'knowledge_graph',
                'type': knowledge_graph.get('type', ''),
                'thumbnail': knowledge_graph.get('thumbnail', '')
            }
            results.insert(0, parsed_result)  # –°—Ç–∞–≤–∏–º –≤ –Ω–∞—á–∞–ª–æ
        
        return results
    
    async def search_news(
        self,
        query: str,
        num_results: int = 10,
        language: str = "ru",
        country: str = "ru"
    ) -> List[Dict[str, Any]]:
        """
        –ü–æ–∏—Å–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            num_results: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            language: –Ø–∑—ã–∫ –ø–æ–∏—Å–∫–∞
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        if not self.api_key:
            logger.error("SERP API –∫–ª—é—á –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")
            return []
        
        try:
            # –£–ª—É—á—à–∞–µ–º –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
            if not query or query.strip() in ['–ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏', '–∞–∫—Ç—É–∞–ª—å–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏', '']:
                query = '–Ω–æ–≤–æ—Å—Ç–∏ –†–æ—Å—Å–∏–∏ —Å–µ–≥–æ–¥–Ω—è'
            
            params = {
                'q': query,
                'api_key': self.api_key,
                'engine': 'google_news',
                'num': num_results,
                'hl': language,
                'gl': country,
                'tbm': 'nws',  # –¢–æ–ª—å–∫–æ –Ω–æ–≤–æ—Å—Ç–∏
                'tbs': 'qdr:d'  # –¢–æ–ª—å–∫–æ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π –¥–µ–Ω—å
            }
            
            if not self.session:
                self.session = aiohttp.ClientSession()
            
            async with self.session.get(self.base_url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    return self._parse_news_results(data)
                else:
                    logger.error(f"–û—à–∏–±–∫–∞ SERP API (–Ω–æ–≤–æ—Å—Ç–∏): {response.status}")
                    return []
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return []
    
    def _filter_spam_news(self, title: str, snippet: str, source: str) -> bool:
        """–§–∏–ª—å—Ç—Ä–æ–≤–∞—Ç—å —Å–ø–∞–º –∏ —Ä–µ–∫–ª–∞–º–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏"""
        spam_keywords = [
            '–±—É–∫–º–µ–∫–µ—Ä', '–±—É–∫–º–µ–∫–µ—Ä', '—Å—Ç–∞–≤–∫–∏', '–ø—Ä–æ–º–æ–∫–æ–¥', '—Ñ—Ä–∏–±–µ—Ç', '–±–æ–Ω—É—Å',
            '–∫–∞–∑–∏–Ω–æ', '–∏–≥—Ä–æ–≤—ã–µ –∞–≤—Ç–æ–º–∞—Ç—ã', '—Å–ª–æ—Ç—ã', '–ø–æ–∫–µ—Ä', '—Ä—É–ª–µ—Ç–∫–∞',
            '–∑–∞—Ä–∞–±–æ—Ç–æ–∫ –≤ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ', '–±—ã—Å—Ç—Ä—ã–µ –¥–µ–Ω—å–≥–∏', '–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏ —Å –≥–∞—Ä–∞–Ω—Ç–∏–µ–π',
            '—Ñ–æ—Ä–µ–∫—Å', '–±–∏–Ω–∞—Ä–Ω—ã–µ –æ–ø—Ü–∏–æ–Ω—ã', '–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–∞ –∑–∞—Ä–∞–±–æ—Ç–æ–∫',
            '–ø–æ—Ö—É–¥–µ–Ω–∏–µ –∑–∞', '–¥–∏–µ—Ç–∞ —á—É–¥–æ', '—É–≤–µ–ª–∏—á–µ–Ω–∏–µ', '–ø–æ—Ç–µ–Ω—Ü–∏—è'
        ]
        
        spam_sources = [
            'bookmaker-ratings.ru', 'legalbet.ru', 'vseprosport.ru',
            'stavka.tv', 'metaratings.ru', 'odds.ru'
        ]
        
        text_to_check = f"{title.lower()} {snippet.lower()}"
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∞–º-–∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        for keyword in spam_keywords:
            if keyword in text_to_check:
                return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–ø–∞–º-–∏—Å—Ç–æ—á–Ω–∏–∫–∏
        for spam_source in spam_sources:
            if spam_source.lower() in source.lower():
                return False
                
        return True
    
    def _parse_news_results(self, data: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        –ü–∞—Ä—Å–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        
        Args:
            data: –°—ã—Ä—ã–µ –¥–∞–Ω–Ω—ã–µ –æ—Ç API
            
        Returns:
            –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        results = []
        
        news_results = data.get('news_results', [])
        for result in news_results:
            title = result.get('title', '')
            snippet = result.get('snippet', '')
            source = result.get('source', '')
            
            # –§–∏–ª—å—Ç—Ä—É–µ–º —Å–ø–∞–º
            if not self._filter_spam_news(title, snippet, source):
                continue
                
            parsed_result = {
                'title': title,
                'link': result.get('link', ''),
                'snippet': snippet,
                'source': source,
                'date': result.get('date', ''),
                'thumbnail': result.get('thumbnail', ''),
                'position': result.get('position', 0)
            }
            results.append(parsed_result)
        
        return results
    
    def format_search_results(self, results: List[Dict[str, Any]], max_results: int = 5, max_length: int = 3500) -> str:
        """
        –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
        
        Args:
            results: –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∏—Å–∫–∞
            max_results: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        if not results:
            return "üîç –ü–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
        
        formatted_text = "üåê **–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤–µ–±-–ø–æ–∏—Å–∫–∞:**\n\n"
        
        for i, result in enumerate(results[:max_results], 1):
            title = result.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:80]
            snippet = result.get('snippet', '')[:200]
            link = result.get('link', '')
            source = result.get('source', 'web')
            
            # –≠–º–æ–¥–∑–∏ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            emoji_map = {
                'answer_box': 'üí°',
                'knowledge_graph': 'üìö',
                'news': 'üì∞',
                'organic': 'üîó'
            }
            emoji = emoji_map.get(source, 'üîó')
            
            formatted_text += f"{emoji} **{i}. {title}**\n"
            
            if snippet:
                formatted_text += f"   {snippet}\n"
            
            if link and len(link) < 100:
                formatted_text += f"   üîó [{link}]({link})\n"
            
            formatted_text += "\n"
        
        # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        total_count = len(results)
        if total_count > max_results:
            formatted_text += f"_–ü–æ–∫–∞–∑–∞–Ω–æ {max_results} –∏–∑ {total_count} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤_\n"
        
        # –û–±—Ä–µ–∑–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –µ—Å–ª–∏ –æ–Ω–æ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω–æ–µ
        if len(formatted_text) > max_length:
            formatted_text = formatted_text[:max_length-100] + "\n\n...\n\nüìù _–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ–∫—Ä–∞—â–µ–Ω—ã –∏–∑-–∑–∞ –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–π Telegram_"
        
        return formatted_text
    
    async def quick_search(self, query: str) -> str:
        """
        –ë—ã—Å—Ç—Ä—ã–π –ø–æ–∏—Å–∫ —Å —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            
        Returns:
            –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        """
        async with self:
            results = await self.search(query, num_results=5)
            return self.format_search_results(results)
    
    async def search_and_summarize(self, query: str, context: str = "") -> str:
        """
        –ü–æ–∏—Å–∫ —Å AI-—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            context: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
            
        Returns:
            –°—É–º–º–∞—Ä–∏–∑–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        async with self:
            results = await self.search(query, num_results=3)
            
            if not results:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø–æ –≤–∞—à–µ–º—É –∑–∞–ø—Ä–æ—Å—É –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
            
            # –°–æ–±–∏—Ä–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            search_info = ""
            for result in results:
                title = result.get('title', '')
                snippet = result.get('snippet', '')
                search_info += f"–ó–∞–≥–æ–ª–æ–≤–æ–∫: {title}\n–û–ø–∏—Å–∞–Ω–∏–µ: {snippet}\n\n"
            
            # –ó–¥–µ—Å—å –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—é —Å OpenAI –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
            # –ü–æ–∫–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—Ä–æ—Å—Ç–æ–π —Ñ–æ—Ä–º–∞—Ç
            summary = f"üîç **–ü–æ–∏—Å–∫ –ø–æ –∑–∞–ø—Ä–æ—Å—É:** {query}\n\n"
            summary += "üìã **–ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞:**\n"
            
            for i, result in enumerate(results[:3], 1):
                title = result.get('title', '')[:60]
                snippet = result.get('snippet', '')[:150]
                summary += f"\n**{i}. {title}**\n{snippet}...\n"
            
            return summary
    
    async def get_daily_news_summary(self, query: str = "–Ω–æ–≤–æ—Å—Ç–∏ –†–æ—Å—Å–∏–∏ —Å–µ–≥–æ–¥–Ω—è") -> str:
        """
        –ü–æ–ª—É—á–∏—Ç—å –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ –¥–µ–Ω—å —Å AI-–∞–Ω–∞–ª–∏–∑–æ–º
        
        Args:
            query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –Ω–æ–≤–æ—Å—Ç–µ–π
            
        Returns:
            –ö—Ä–∞—Ç–∫–∞—è —Å–≤–æ–¥–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
        """
        try:
            async with self:
                # –ü–æ–ª—É—á–∞–µ–º –±–æ–ª—å—à–µ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –ª—É—á—à–µ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                news_results = await self.search_news(query, num_results=15)
                
                if not news_results:
                    return "üì∞ –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –∞–∫—Ç—É–∞–ª—å–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Å–µ–≥–æ–¥–Ω—è –Ω–µ –Ω–∞–π–¥–µ–Ω–æ."
                
                # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–ø-5 –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π
                top_news = news_results[:5]
                
                # –°–æ–±–∏—Ä–∞–µ–º –∫—Ä–∞—Ç–∫—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ—Å—Ç—è—Ö
                news_summary = "üì∞ **–°–≤–æ–¥–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Å–µ–≥–æ–¥–Ω—è:**\n\n"
                
                for i, news in enumerate(top_news, 1):
                    title = news.get('title', '–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è')[:100]
                    source = news.get('source', '–ò—Å—Ç–æ—á–Ω–∏–∫ –Ω–µ —É–∫–∞–∑–∞–Ω')
                    date = news.get('date', '')
                    
                    news_summary += f"**{i}. {title}**\n"
                    if source:
                        news_summary += f"   üì° {source}"
                    if date:
                        news_summary += f" | üìÖ {date}"
                    news_summary += "\n\n"
                
                # –ï—Å–ª–∏ AI –¥–æ—Å—Ç—É–ø–µ–Ω, –¥–æ–±–∞–≤–ª—è–µ–º –∫—Ä–∞—Ç–∫–∏–π –∞–Ω–∞–ª–∏–∑
                try:
                    from utils.openai_client import openai_client
                    
                    # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–æ–≤–æ—Å—Ç–µ–π
                    prompt = f"""–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏ —Å–æ–∑–¥–∞–π –∫—Ä–∞—Ç–∫—É—é —Å–≤–æ–¥–∫—É (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è) –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π –¥–Ω—è:

{chr(10).join([f"- {news.get('title', '')}: {news.get('snippet', '')[:100]}" for news in top_news])}

–°–≤–æ–¥–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∫—Ä–∞—Ç–∫–æ–π –∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–π."""

                    ai_summary = await openai_client.get_response(prompt, system_message="–¢—ã - –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫. –°–æ–∑–¥–∞–≤–∞–π –∫—Ä–∞—Ç–∫–∏–µ –∏ —Ç–æ—á–Ω—ã–µ —Å–≤–æ–¥–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π.")
                    
                    news_summary += f"ü§ñ **AI-–∞–Ω–∞–ª–∏–∑ –¥–Ω—è:**\n{ai_summary}\n\n"
                    
                except Exception as ai_error:
                    logger.warning(f"AI-–∞–Ω–∞–ª–∏–∑ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {ai_error}")
                
                news_summary += f"üìä _–ü–æ–∫–∞–∑–∞–Ω–æ {len(top_news)} –∏–∑ {len(news_results)} –Ω–æ–≤–æ—Å—Ç–µ–π_"
                
                return news_summary
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
            return "‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–≤–æ–¥–∫–∏ –Ω–æ–≤–æ—Å—Ç–µ–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ."

# –°–æ–∑–¥–∞–µ–º –≥–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä
web_searcher = WebSearcher()